########################
# Which model to use   #
########################

http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html

#############################
# Confusion Matrix          #
#############################

import sklearn.metrics as metrics
y_true = [1, 1, 2, 2, 3, 3]  # Actual, observed testing dataset values
y_pred = [1, 1, 1, 3, 2, 3]  # Predicted values from your model
metrics.confusion_matrix(y_true, y_pred)

Predicted across the top, Actual along the left

 	Predicted Cat 	Predicted Dog 	Predicted Monkey
Actual Cat 	2 	0 	0
Actual Dog 	1 	0 	1
Actual Monkey 	0 	1 	1

###########################
# Scoring Metrics         #
###########################

import sklearn.metrics as metrics
y_true = [1, 1, 2, 2, 3, 3]  # Actual, observed testing datset values
y_pred = [1, 1, 1, 3, 2, 3]  # Predicted values from your model

metrics.accuracy_score(y_true, y_pred)
0.5

metrics.accuracy_score(y_true, y_pred, normalize=False)
3

metrics.recall_score(y_true, y_pred, average='weighted')
0.5

metrics.recall_score(y_true, y_pred, average=None)
array([ 1. ,  0. ,  0.5])

metrics.precision_score(y_true, y_pred, average='weighted')
0.38888888888888884

metrics.precision_score(y_true, y_pred, average=None)
array([ 0.66666667,  0.        ,  0.5       ])

metrics.f1_score(y_true, y_pred, average='weighted')
0.43333333333333335

metrics.f1_score(y_true, y_pred, average=None)
array([ 0.8,  0. ,  0.5])

Full Report

As a convenience, SciKit-Learn also has a built-in method for producing a full report of the above scores for you simultaneously, on a per label basis:

target_names = ['Fruit 1', 'Fruit 2', 'Fruit 3']
metrics.classification_report(y_true, y_pred, target_names=target_names)


             precision    recall  f1-score   support

    Fruit 1       0.67      1.00      0.80         2
    Fruit 2       0.00      0.00      0.00         2
    Fruit 3       0.50      0.50      0.50         2

avg / total       0.39      0.50      0.43         6

